{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/briankhor/amazon-review-sentiment-analysis?scriptVersionId=110733049\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Amazon Review Sentiment Analysis\n\nThe Amazon review dataset consists of texts of reviews by customer and has been classified as positive or negative. The dataset would be largely balanced (as we would expect in real life situations where there are many ratings, both positive and negative, on the Amazon webpage). This makes this dataset the perfect case studying for applying natural language processing (NLP) framework.\n\nWe will start off by loading the necessary Python libraries.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.python.keras import models, layers, optimizers\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport bz2\nfrom sklearn.metrics import f1_score, roc_auc_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport re\n\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"../input\"))","metadata":{"execution":{"iopub.status.busy":"2022-11-11T20:01:21.437238Z","iopub.execute_input":"2022-11-11T20:01:21.437763Z","iopub.status.idle":"2022-11-11T20:01:28.37948Z","shell.execute_reply.started":"2022-11-11T20:01:21.437668Z","shell.execute_reply":"2022-11-11T20:01:28.377185Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"['amazonreviews']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Reading the text (converting raw binary strings to strings that can be parsed)\n\nWe need to read the bz2 files in the dataset. The text is in raw binary compressed format. We need to process our data. The data is given in the fornat of label (good or bad) followed by text so the first word needs to be converted to a number (rating 1-2 as bad and 4-5 as good) and the rest as texts.","metadata":{}},{"cell_type":"code","source":"def separate_label_and_text(file):\n    labels = []\n    texts = []\n    for line in bz2.BZ2File(file):\n        x = line.decode(\"utf-8\")\n        labels.append(int(x[9]) - 1)\n        texts.append(x[10:].strip())\n    return np.array(labels), texts\n\ntrain_labels, train_texts = separate_label_and_text('../input/amazonreviews/train.ft.txt.bz2')\ntest_labels, test_texts = separate_label_and_text('../input/amazonreviews/test.ft.txt.bz2')","metadata":{"execution":{"iopub.status.busy":"2022-11-11T20:01:32.516649Z","iopub.execute_input":"2022-11-11T20:01:32.517289Z","iopub.status.idle":"2022-11-11T20:03:42.908889Z","shell.execute_reply.started":"2022-11-11T20:01:32.517253Z","shell.execute_reply":"2022-11-11T20:03:42.907925Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Text Processing\n\nNext, we need to process the texts input given by users. We will apply lowercase to all letters, and replace any non words characters with spaces. We will also remove any characters with accents in this rather simplified text processing.","metadata":{}},{"cell_type":"code","source":"NON_ALPHANUM = re.compile(r'[\\W]')\nNON_ASCII = re.compile(r'[^a-z0-1\\s]')\n\ndef text_processing(texts):\n    processed_texts = []\n    for text in texts:\n        lower = text.lower()\n        no_punctuation = NON_ALPHANUM.sub(r' ', lower)\n        no_non_ascii = NON_ASCII.sub(r'', no_punctuation)\n        processed_texts.append(no_non_ascii)\n    return processed_texts\n\ntrain_texts = text_processing(train_texts)\ntest_texts = text_processing(test_texts)","metadata":{"execution":{"iopub.status.busy":"2022-11-11T20:03:49.21614Z","iopub.execute_input":"2022-11-11T20:03:49.216537Z","iopub.status.idle":"2022-11-11T20:05:49.017149Z","shell.execute_reply.started":"2022-11-11T20:03:49.216503Z","shell.execute_reply":"2022-11-11T20:05:49.016158Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Splitting Test/Dev Set\n\nWe have the training and test sets, but we can further split the train dataset into train (80%) and development set (20%).","metadata":{}},{"cell_type":"code","source":"train_texts, dev_texts, train_labels, dev_labels = train_test_split(train_texts, train_labels, random_state = 328413, test_size = 0.2)","metadata":{"execution":{"iopub.status.busy":"2022-11-11T20:05:55.970165Z","iopub.execute_input":"2022-11-11T20:05:55.97121Z","iopub.status.idle":"2022-11-11T20:05:57.46256Z","shell.execute_reply.started":"2022-11-11T20:05:55.971154Z","shell.execute_reply":"2022-11-11T20:05:57.461515Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"We will now run tokenizers to choose the 12000 most used words as features below.","metadata":{}},{"cell_type":"code","source":"MAX_FEATURES = 12000\ntokenizer = Tokenizer(num_words=MAX_FEATURES)\ntokenizer.fit_on_texts(train_texts)\ntrain_texts = tokenizer.texts_to_sequences(train_texts)\ndev_texts = tokenizer.texts_to_sequences(dev_texts)\ntest_texts = tokenizer.texts_to_sequences(test_texts)","metadata":{"execution":{"iopub.status.busy":"2022-11-11T20:06:03.424324Z","iopub.execute_input":"2022-11-11T20:06:03.424702Z","iopub.status.idle":"2022-11-11T20:12:59.783839Z","shell.execute_reply.started":"2022-11-11T20:06:03.424655Z","shell.execute_reply":"2022-11-11T20:12:59.782747Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Padding sequences\n\nTo use batches effectively, we will add padding to shorter sentences so that all sentences have lengths matching that of the longest sentence. ","metadata":{}},{"cell_type":"code","source":"MAX_LENGTH = max(len(train_ex) for train_ex in train_texts)\ntrain_texts = pad_sequences(train_texts, maxlen = MAX_LENGTH)\ndev_texts = pad_sequences(dev_texts, maxlen = MAX_LENGTH)\ntest_texts = pad_sequences(test_texts, maxlen = MAX_LENGTH)","metadata":{"execution":{"iopub.status.busy":"2022-11-11T20:15:50.239795Z","iopub.execute_input":"2022-11-11T20:15:50.240157Z","iopub.status.idle":"2022-11-11T20:16:22.760784Z","shell.execute_reply.started":"2022-11-11T20:15:50.240125Z","shell.execute_reply":"2022-11-11T20:16:22.75974Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Recurrent Neural Network (RNN) Model\n\nWe are now ready to apply the RNN model to our dataset. I will use a simple model with embedding, 2 GRU layers, and 2 dense layers followed by the output layer.","metadata":{}},{"cell_type":"code","source":"def make_rnn_model():\n    sequences = layers.Input(shape=(MAX_LENGTH, ))\n    embedded = layers.Embedding(MAX_FEATURES, 64)(sequences)\n    x = layers.CuDNNGRU(128, return_sequences=True)(embedded)\n    x = layers.CuDNNGRU(128)(x)\n    x = layers.Dense(32, activation='relu')(x)\n    x = layers.Dense(100, activation='relu')(x)\n    predictions = layers.Dense(1, activation='sigmoid')(x)\n    model = models.Model(inputs = sequences, outputs = predictions)\n    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])\n    return model\n\nrnn_model = make_rnn_model()","metadata":{"execution":{"iopub.status.busy":"2022-11-11T20:16:32.360739Z","iopub.execute_input":"2022-11-11T20:16:32.361106Z","iopub.status.idle":"2022-11-11T20:16:35.591758Z","shell.execute_reply.started":"2022-11-11T20:16:32.361073Z","shell.execute_reply":"2022-11-11T20:16:35.59074Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2022-11-11 20:16:32.471004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-11 20:16:32.616430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-11 20:16:32.617368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-11 20:16:32.619400: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-11-11 20:16:32.619765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-11 20:16:32.620522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-11 20:16:32.621214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-11 20:16:34.836264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-11 20:16:34.837734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-11 20:16:34.838966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-11 20:16:34.840043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We apply the RNN model to our train and development dataset","metadata":{}},{"cell_type":"code","source":"rnn_model.fit(train_texts, train_labels, batch_size=256, epochs=1, validation_data=(dev_texts, dev_labels), )","metadata":{"execution":{"iopub.status.busy":"2022-11-11T20:16:46.740925Z","iopub.execute_input":"2022-11-11T20:16:46.741832Z","iopub.status.idle":"2022-11-11T20:27:12.761714Z","shell.execute_reply.started":"2022-11-11T20:16:46.741795Z","shell.execute_reply":"2022-11-11T20:27:12.760727Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"2022-11-11 20:16:46.746342: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 2937600000 exceeds 10% of free system memory.\n2022-11-11 20:16:49.407404: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n2022-11-11 20:16:52.041407: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n","output_type":"stream"},{"name":"stdout","text":"11250/11250 [==============================] - ETA: 0s - loss: 0.1681 - binary_accuracy: 0.9354","output_type":"stream"},{"name":"stderr","text":"2022-11-11 20:26:11.967361: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 734400000 exceeds 10% of free system memory.\n","output_type":"stream"},{"name":"stdout","text":"11250/11250 [==============================] - 610s 54ms/step - loss: 0.1681 - binary_accuracy: 0.9354 - val_loss: 0.1556 - val_binary_accuracy: 0.9426\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7fe6a7164e90>"},"metadata":{}}]},{"cell_type":"markdown","source":"We will finally test this model with our test set","metadata":{}},{"cell_type":"code","source":"preds = rnn_model.predict(test_texts)\nprint('Accuracy Score: {}'.format(accuracy_score(test_labels, 1*(preds>0.5) ) ) )\nprint('F1 Score: {}'.format(f1_score(test_labels, 1*(preds>0.5) ) ) )\nprint('ROC AUC Score: {}'.format(roc_auc_score(test_labels, preds) ) )","metadata":{"execution":{"iopub.status.busy":"2022-11-11T20:39:14.63027Z","iopub.execute_input":"2022-11-11T20:39:14.630627Z","iopub.status.idle":"2022-11-11T20:40:54.552404Z","shell.execute_reply.started":"2022-11-11T20:39:14.630595Z","shell.execute_reply":"2022-11-11T20:40:54.551384Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"2022-11-11 20:39:14.636722: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 408000000 exceeds 10% of free system memory.\n","output_type":"stream"},{"name":"stdout","text":"Accuracy Score: 0.94241\nF1 Score: 0.9407954930967483\nROC AUC Score: 0.9871339611000001\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## References\n\n1. [Sentiment Analysis with Amazon Reviews](http://www.kaggle.com/code/muonneutrino/sentiment-analysis-with-amazon-reviews/notebook)\n\n2. [CuDNNLSTM Implementation (93.7% Accuracy)](http://www.kaggle.com/code/anshulrai/cudnnlstm-implementation-93-7-accuracy/notebook)","metadata":{}}]}